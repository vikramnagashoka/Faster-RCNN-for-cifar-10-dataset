{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "obj_centre = []\n",
    "obj_width = []\n",
    "lines = []\n",
    "lines = [line.rstrip('\\n') for line in open(\"./cifar10_transformed/devkit/train.txt\")]\n",
    "obj_centre = []\n",
    "obj_width = []\n",
    "\n",
    "\n",
    "lines = np.asarray(lines)\n",
    "print(lines[0])\n",
    "for i in range(len(lines)):\n",
    "    obj_centre.append([lines[i][12:15],lines[i][15:18]])\n",
    "    obj_width.append(lines[i][18:])\n",
    "\n",
    "obj_centre = np.asarray(obj_centre)\n",
    "obj_width = np.asarray(obj_width)\n",
    "print(obj_centre.shape)\n",
    "print(obj_width.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "from scipy import misc\n",
    "files = glob.glob(\"./cifar10_transformed/masks/*.png\")\n",
    "masks_reshaped = []\n",
    "for i in range(len(files)):\n",
    "    temp_mask = misc.imread(files[i])\n",
    "    masks_reshaped.append(np.ravel(temp_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "file_data = glob.glob(\"./cifar10_transformed/imgs/*.png\")\n",
    "for i in range(len(file_data)):\n",
    "    name = file_data[i]\n",
    "    data.append(misc.imread(name))\n",
    "\n",
    "data = np.asarray(data)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "lines = [line.rstrip('\\n') for line in open(\"./cifar10_transformed/devkit/test.txt\")]\n",
    "rows = []\n",
    "columns = []\n",
    "width = []\n",
    "\n",
    "\n",
    "lines = np.asarray(lines)\n",
    "print(lines[0])\n",
    "for i in range(len(lines)):\n",
    "    rows.append(lines[i][12:15])\n",
    "    columns.append(lines[i][15:18])\n",
    "    width.append(lines[i][18:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dimensions = np.vstack([rows,columns,width])\n",
    "\n",
    "dimensions = dimensions.transpose()\n",
    "print(dimensions.shape)\n",
    "\n",
    "dimensions_test = np.vstack([rows,columns,width])\n",
    "\n",
    "dimensions_test = dimensions_test.transpose()\n",
    "print(dimensions_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_labels = [int(line[10:11]) for line in open(\"./cifar10_transformed/devkit/train.txt\")]\n",
    "testing_labels = [int(line[10:11]) for line in open(\"./cifar10_transformed/devkit/test.txt\")]\n",
    "dimensions_stack = np.vstack([dimensions,dimensions_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "x = tf.placeholder(tf.float32, shape=[None, 48,48,3])\n",
    "\n",
    "def batch_norm(x, is_train=True, decay=0.99, epsilon=0.001):\n",
    "    shape_x = x.get_shape().as_list()\n",
    "    beta = tf.get_variable('beta', shape_x[-1], initializer=tf.constant_initializer(0.0))\n",
    "    gamma = tf.get_variable('gamma', shape_x[-1], initializer=tf.constant_initializer(1.0))\n",
    "    moving_mean = tf.get_variable('moving_mean', shape_x[-1],\n",
    "                initializer=tf.constant_initializer(0.0), trainable=False)\n",
    "    moving_var = tf.get_variable('moving_var', shape_x[-1],\n",
    "               initializer=tf.constant_initializer(1.0), trainable=False)\n",
    "\n",
    "    if is_train:\n",
    "        mean, var = tf.nn.moments(x, np.arange(len(shape_x)-1), keep_dims=True)\n",
    "        mean = tf.reshape(mean, [mean.shape.as_list()[-1]])\n",
    "        #mean = tf.reshape(mean, [mean.get_shape().as_list()[-1]])\n",
    "        var = tf.reshape(var, [var.shape.as_list()[-1]])\n",
    "\n",
    "        update_moving_mean = tf.assign(moving_mean, moving_mean*decay + mean*(1-decay))\n",
    "        update_moving_var = tf.assign(moving_var,\n",
    "                            moving_var*decay + shape_x[0]/(shape_x[0]-1)*var*(1-decay))\n",
    "        update_ops = [update_moving_mean, update_moving_var]\n",
    "\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            return tf.nn.batch_normalization(x, mean, var, beta, gamma, epsilon)\n",
    "\n",
    "    else:\n",
    "        mean = moving_mean\n",
    "        var = moving_var\n",
    "        return tf.nn.batch_normalization(x, mean, var, beta, gamma, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def transformer(U, theta, out_size, name='SpatialTransformer', **kwargs):\n",
    "    \"\"\"Spatial Transformer Layer\n",
    "    Implements a spatial transformer layer as described in [1]_.\n",
    "    Based on [2]_ and edited by David Dao for Tensorflow.\n",
    "    Parameters\n",
    "    ----------\n",
    "    U : float\n",
    "        The output of a convolutional net should have the\n",
    "        shape [num_batch, height, width, num_channels].\n",
    "    theta: float\n",
    "        The output of the\n",
    "        localisation network should be [num_batch, 6].\n",
    "    out_size: tuple of two ints\n",
    "        The size of the output of the network (height, width)\n",
    "    References\n",
    "    ----------\n",
    "    .. [1]  Spatial Transformer Networks\n",
    "            Max Jaderberg, Karen Simonyan, Andrew Zisserman, Koray Kavukcuoglu\n",
    "            Submitted on 5 Jun 2015\n",
    "    .. [2]  https://github.com/skaae/transformer_network/blob/master/transformerlayer.py\n",
    "    Notes\n",
    "    -----\n",
    "    To initialize the network to the identity transform init\n",
    "    ``theta`` to :\n",
    "        identity = np.array([[1., 0., 0.],\n",
    "                             [0., 1., 0.]])\n",
    "        identity = identity.flatten()\n",
    "        theta = tf.Variable(initial_value=identity)\n",
    "    \"\"\"\n",
    "\n",
    "    def _repeat(x, n_repeats):\n",
    "        with tf.variable_scope('_repeat'):\n",
    "            rep = tf.transpose(\n",
    "                tf.expand_dims(tf.ones(shape=tf.stack([n_repeats, ])), 1), [1, 0])\n",
    "            rep = tf.cast(rep, 'int32')\n",
    "            x = tf.matmul(tf.reshape(x, (-1, 1)), rep)\n",
    "            return tf.reshape(x, [-1])\n",
    "\n",
    "    def _interpolate(im, x, y, out_size):\n",
    "        with tf.variable_scope('_interpolate'):\n",
    "            # constants\n",
    "            num_batch = tf.shape(im)[0]\n",
    "            height = tf.shape(im)[1]\n",
    "            width = tf.shape(im)[2]\n",
    "            channels = tf.shape(im)[3]\n",
    "\n",
    "            x = tf.cast(x, 'float32')\n",
    "            y = tf.cast(y, 'float32')\n",
    "            height_f = tf.cast(height, 'float32')\n",
    "            width_f = tf.cast(width, 'float32')\n",
    "            out_height = out_size[0]\n",
    "            out_width = out_size[1]\n",
    "            zero = tf.zeros([], dtype='int32')\n",
    "            max_y = tf.cast(tf.shape(im)[1] - 1, 'int32')\n",
    "            max_x = tf.cast(tf.shape(im)[2] - 1, 'int32')\n",
    "\n",
    "            # scale indices from [-1, 1] to [0, width/height]\n",
    "            x = (x + 1.0)*(width_f-1-1e-6) / 2.0\n",
    "            y = (y + 1.0)*(height_f-1-1e-6) / 2.0\n",
    "\n",
    "            # do sampling\n",
    "            x0 = tf.cast(tf.floor(x), 'int32')\n",
    "            x1 = x0 + 1\n",
    "            y0 = tf.cast(tf.floor(y), 'int32')\n",
    "            y1 = y0 + 1\n",
    "\n",
    "            x0 = tf.clip_by_value(x0, zero, max_x)\n",
    "            x1 = tf.clip_by_value(x1, zero, max_x)\n",
    "            y0 = tf.clip_by_value(y0, zero, max_y)\n",
    "            y1 = tf.clip_by_value(y1, zero, max_y)\n",
    "            dim2 = width\n",
    "            dim1 = width*height\n",
    "            base = _repeat(tf.range(num_batch)*dim1, out_height*out_width)\n",
    "            base_y0 = base + y0*dim2\n",
    "            base_y1 = base + y1*dim2\n",
    "            idx_a = base_y0 + x0\n",
    "            idx_b = base_y1 + x0\n",
    "            idx_c = base_y0 + x1\n",
    "            idx_d = base_y1 + x1\n",
    "\n",
    "            # use indices to lookup pixels in the flat image and restore\n",
    "            # channels dim\n",
    "            im_flat = tf.reshape(im, tf.stack([-1, channels]))\n",
    "            im_flat = tf.cast(im_flat, 'float32')\n",
    "            Ia = tf.gather(im_flat, idx_a)\n",
    "            Ib = tf.gather(im_flat, idx_b)\n",
    "            Ic = tf.gather(im_flat, idx_c)\n",
    "            Id = tf.gather(im_flat, idx_d)\n",
    "\n",
    "            # and finally calculate interpolated values\n",
    "            x0_f = tf.cast(x0, 'float32')\n",
    "            x1_f = tf.cast(x1, 'float32')\n",
    "            y0_f = tf.cast(y0, 'float32')\n",
    "            y1_f = tf.cast(y1, 'float32')\n",
    "            wa = tf.expand_dims(((x1_f-x) * (y1_f-y)), 1)\n",
    "            wb = tf.expand_dims(((x1_f-x) * (y-y0_f)), 1)\n",
    "            wc = tf.expand_dims(((x-x0_f) * (y1_f-y)), 1)\n",
    "            wd = tf.expand_dims(((x-x0_f) * (y-y0_f)), 1)\n",
    "            output = tf.add_n([wa*Ia, wb*Ib, wc*Ic, wd*Id])\n",
    "            return output\n",
    "\n",
    "    def _meshgrid(height, width):\n",
    "        with tf.variable_scope('_meshgrid'):\n",
    "            # This should be equivalent to:\n",
    "            #  x_t, y_t = np.meshgrid(np.linspace(-1, 1, width),\n",
    "            #                         np.linspace(-1, 1, height))\n",
    "            #  ones = np.ones(np.prod(x_t.shape))\n",
    "            #  grid = np.vstack([x_t.flatten(), y_t.flatten(), ones])\n",
    "            x_t = tf.matmul(tf.ones(shape=tf.stack([height, 1])),\n",
    "                            tf.transpose(tf.expand_dims(tf.linspace(-1.0, 1.0, width), 1), [1, 0]))\n",
    "            y_t = tf.matmul(tf.expand_dims(tf.linspace(-1.0, 1.0, height), 1),\n",
    "                            tf.ones(shape=tf.stack([1, width])))\n",
    "\n",
    "            x_t_flat = tf.reshape(x_t, (1, -1))\n",
    "            y_t_flat = tf.reshape(y_t, (1, -1))\n",
    "\n",
    "            ones = tf.ones_like(x_t_flat)\n",
    "            grid = tf.concat(axis=0, values=[x_t_flat, y_t_flat, ones])\n",
    "            return grid\n",
    "\n",
    "    def _transform(theta, input_dim, out_size):\n",
    "        with tf.variable_scope('_transform'):\n",
    "            num_batch = tf.shape(input_dim)[0]\n",
    "            height = tf.shape(input_dim)[1]\n",
    "            width = tf.shape(input_dim)[2]\n",
    "            num_channels = tf.shape(input_dim)[3]\n",
    "            theta = tf.reshape(theta, (-1, 2, 3))\n",
    "            theta = tf.cast(theta, 'float32')\n",
    "\n",
    "            # grid of (x_t, y_t, 1), eq (1) in ref [1]\n",
    "            height_f = tf.cast(height, 'float32')\n",
    "            width_f = tf.cast(width, 'float32')\n",
    "            out_height = out_size[0]\n",
    "            out_width = out_size[1]\n",
    "            grid = _meshgrid(out_height, out_width)\n",
    "            grid = tf.expand_dims(grid, 0)\n",
    "            grid = tf.reshape(grid, [-1])\n",
    "            grid = tf.tile(grid, tf.stack([num_batch]))\n",
    "            grid = tf.reshape(grid, tf.stack([num_batch, 3, -1]))\n",
    "\n",
    "            # Transform A x (x_t, y_t, 1)^T -> (x_s, y_s)\n",
    "            T_g = tf.matmul(theta, grid)\n",
    "            x_s = tf.slice(T_g, [0, 0, 0], [-1, 1, -1])\n",
    "            y_s = tf.slice(T_g, [0, 1, 0], [-1, 1, -1])\n",
    "            x_s_flat = tf.reshape(x_s, [-1])\n",
    "            y_s_flat = tf.reshape(y_s, [-1])\n",
    "\n",
    "            input_transformed = _interpolate(\n",
    "                input_dim, x_s_flat, y_s_flat,\n",
    "                out_size)\n",
    "\n",
    "            output = tf.reshape(\n",
    "                input_transformed, tf.stack([num_batch, out_height, out_width, num_channels]))\n",
    "            return output\n",
    "\n",
    "    with tf.variable_scope(name):\n",
    "        output = _transform(theta, U, out_size)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import flatten\n",
    "from tensorflow.contrib.layers.python.layers import batch_norm as batch_norm\n",
    "slim = tf.contrib.slim\n",
    "mu = 0\n",
    "sigma = 0.1\n",
    "\n",
    "img_index = 0\n",
    "def cifarNet(x, y, dims, is_train=True):\n",
    "    two = tf.constant(2,tf.float32)\n",
    "    one = tf.constant(1,tf.float32)\n",
    "    vs = tf.get_variable_scope()\n",
    "    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 3, 32), mean = mu, stddev = sigma))\n",
    "    conv1_b = tf.Variable(tf.zeros(32))\n",
    "    conv1   = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='SAME') + conv1_b\n",
    "    conv1 = batch_norm(conv1)\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    #Layer 2\n",
    "    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 32, 64), mean = mu, stddev = sigma))\n",
    "    conv2_b = tf.Variable(tf.zeros(64))\n",
    "    conv2   = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='SAME') + conv2_b\n",
    "    conv2 = batch_norm(conv2)\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    #Layer 3\n",
    "    conv3_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 64, 128), mean = mu, stddev = sigma))\n",
    "    conv3_b = tf.Variable(tf.zeros(128))\n",
    "    conv3   = tf.nn.conv2d(conv2, conv3_W, strides=[1, 1, 1, 1], padding='SAME') + conv3_b\n",
    "    \n",
    "    conv3 = batch_norm(conv3)\n",
    "    conv3 = tf.nn.relu(conv3)\n",
    "    conv3 = tf.nn.max_pool(conv3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    #Layer 4\n",
    "    \n",
    "    conv4_W = tf.Variable(tf.truncated_normal(shape=(3, 3, 128, 256), mean = mu, stddev = sigma))\n",
    "    conv4_b = tf.Variable(tf.zeros(256))\n",
    "    \n",
    "    conv4   = tf.nn.conv2d(conv3, conv4_W, strides=[1, 1, 1, 1], padding='SAME') + conv4_b\n",
    "    conv4 = batch_norm(conv4)\n",
    "    \n",
    "    conv4 = tf.nn.relu(conv4)\n",
    "    \n",
    "    #Layer 5 intermediate layer\n",
    "    conv5_W = tf.Variable(tf.truncated_normal(shape=(3, 3, 256, 256), mean = mu, stddev = sigma))\n",
    "    conv5_b = tf.Variable(tf.zeros(256))\n",
    "    \n",
    "    conv5   = tf.nn.conv2d(conv4, conv5_W, strides=[1, 1, 1, 1], padding='SAME') + conv5_b\n",
    "    conv5 = batch_norm(conv5)\n",
    "    conv5 = tf.nn.relu(conv5)\n",
    "    \n",
    "    #Layer 6 for classification\n",
    "    conv6_W = tf.Variable(tf.truncated_normal(shape=(1, 1, 256, 1), mean = mu, stddev = sigma))\n",
    "    conv6_b = tf.Variable(tf.zeros(1))\n",
    "    \n",
    "    conv6   = tf.nn.conv2d(conv5, conv6_W,strides=[1, 1, 1, 1], padding='SAME') + conv6_b\n",
    "    conv6 =  flatten(conv6)\n",
    "   \n",
    "    bool_indices = tf.not_equal(y, two)\n",
    "    threshold_indices = tf.where(bool_indices)\n",
    "    ground_truth = tf.gather_nd(y, threshold_indices)\n",
    "    predicted = tf.gather_nd(conv6,threshold_indices)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Layer 7 for regression\n",
    "    conv7_W = tf.Variable(tf.truncated_normal(shape=(1, 1, 256, 3), mean = mu, stddev = sigma))\n",
    "    conv7_b = tf.Variable([24,24,32], dtype=tf.float32)\n",
    "    \n",
    "    conv7   = tf.nn.conv2d(conv5, conv7_W,strides=[1, 1, 1, 1], padding='SAME') + conv7_b\n",
    "\n",
    "    \n",
    "    bool_indices_ones = tf.equal(y, one)\n",
    "    threshold_indices_ones = tf.where(bool_indices_ones)\n",
    "    \n",
    "    conv7 = tf.reshape(conv7,[100,36,3])\n",
    "    predicted_rows = tf.slice(conv7,[0,0,0], [100,36,1])\n",
    "    predicted_rows = tf.reshape(predicted_rows,[100,36])\n",
    "\n",
    "\n",
    "    predicted_columns = tf.slice(conv7,[0,0,1],[100,36,1])\n",
    "    predicted_columns = tf.reshape(predicted_columns,[100,36])\n",
    "\n",
    "    \n",
    "    predicted_width = tf.slice(conv7,[0,0,2],[100,36,1])\n",
    "    predicted_width = tf.reshape(predicted_width,[100,36])\n",
    "\n",
    "    \n",
    "    predicted_rows_dimensions = tf.gather_nd(predicted_rows, threshold_indices_ones)\n",
    "    predicted_columns_dimensions = tf.gather_nd(predicted_columns, threshold_indices_ones)\n",
    "    predicted_width_dimensions = tf.gather_nd(predicted_width, threshold_indices_ones)\n",
    "    \n",
    "    anchor_size = tf.constant(32,dtype=tf.float32)\n",
    "    \n",
    "    predicted_rows_dimensions = predicted_rows_dimensions / anchor_size\n",
    "    predicted_rows_dimensions = tf.reshape(predicted_rows_dimensions,[-1,1])\n",
    "    \n",
    "    predicted_columns_dimensions = predicted_columns_dimensions / anchor_size\n",
    "    predicted_columns_dimensions = tf.reshape(predicted_columns_dimensions,[-1,1])\n",
    "    \n",
    "    \n",
    "    predicted_width_dimensions = predicted_width_dimensions / anchor_size\n",
    "    predicted_width_dimensions = tf.reshape(predicted_width_dimensions,[-1,1])\n",
    "    \n",
    "    \n",
    "    row_dims = tf.slice(dims,[0,0],[100, 1])\n",
    "    row_dims = tf.tile(row_dims,[1,36])\n",
    "    rows_dims = tf.gather_nd(row_dims, threshold_indices_ones)\n",
    "    rows_dims = rows_dims / anchor_size\n",
    "    rows_dims = tf.reshape(rows_dims,[-1,1])\n",
    "    \n",
    "    \n",
    "    column_dims = tf.slice(dims,[0,1],[100, 1])\n",
    "    column_dims = tf.tile(column_dims,[1,36])\n",
    "    columns_dims = tf.gather_nd(column_dims, threshold_indices_ones)\n",
    "    columns_dims = columns_dims / anchor_size\n",
    "    columns_dims = tf.reshape(columns_dims,[-1,1])\n",
    "    \n",
    "    w_dims = tf.slice(dims,[0,2],[100, 1])\n",
    "    w_dims = tf.tile(w_dims,[1,36])\n",
    "    width_dims = tf.gather_nd(w_dims, threshold_indices_ones)\n",
    "    width_dims = width_dims / anchor_size\n",
    "    width_dims = tf.reshape(width_dims,[-1,1])\n",
    "    \n",
    "    ground_dims = tf.concat([rows_dims, columns_dims,width_dims], axis=1)\n",
    "    \n",
    "\n",
    "    predicted_dimensions = tf.concat([predicted_rows_dimensions, predicted_columns_dimensions, predicted_width_dimensions],axis=1)\n",
    "\n",
    "    \n",
    "    ##Object classifier\n",
    "\n",
    "\n",
    "    fc0   = flatten(conv4)\n",
    "\n",
    "    fc1_W = tf.Variable(tf.truncated_normal(shape=(9216, 256), mean = mu, stddev = sigma))\n",
    "    fc1_b = tf.Variable(tf.zeros(256))\n",
    "    fc1   = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "\n",
    "    fc1 = batch_norm(fc1)\n",
    "\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    \n",
    "    fc2_W = tf.Variable(tf.truncated_normal(shape=(256, 10), mean = mu, stddev = sigma))\n",
    "    fc2_b = tf.Variable(tf.zeros(10))\n",
    "    fc2   = tf.matmul(fc1, fc2_W) + fc2_b\n",
    "    logits = fc2\n",
    "    \n",
    "    \n",
    "    return conv6, ground_truth,predicted, predicted_dimensions, ground_dims,logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "EPOCHS = 1500\n",
    "rate_initial = 0.001\n",
    "is_train = True\n",
    "batch = tf.Variable(0)\n",
    "y = tf.placeholder(tf.float32, shape=[None,36])\n",
    "w = tf.placeholder(tf.float32, shape=[None,6,6])\n",
    "dims = tf.placeholder(tf.float32, shape=[None,3])\n",
    "ground_dimensions = tf.placeholder(tf.float32, shape=[None,6,3])\n",
    "\n",
    "pred_label = tf.placeholder(tf.int32, (None))\n",
    "one_hot_y = tf.one_hot(pred_label, 10)\n",
    "\n",
    "logits, ground_truth ,predicted, predicted_dimensions,ground_dims, logits_class = cifarNet(x, y, dims, is_train)\n",
    "\n",
    "predicted_temp = tf.nn.sigmoid(predicted)\n",
    "predicted_threshold = tf.greater(predicted_temp, 0.5)\n",
    "predicted_threshold = tf.cast(predicted_threshold, tf.int32)\n",
    "ground_truth_acc = tf.nn.sigmoid(ground_truth)\n",
    "predicted_threshold = tf.cast(predicted_threshold, tf.float32)\n",
    "\n",
    "\n",
    "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=ground_truth, logits=predicted)\n",
    "\n",
    "dimensions_loss = tf.losses.huber_loss(predicted_dimensions, ground_dims)\n",
    "\n",
    "\n",
    "cross_entropy_classification = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits_class)\n",
    "classification_loss = tf.reduce_mean(cross_entropy_classification)\n",
    "\n",
    "\n",
    "loss_operation = tf.reduce_mean(cross_entropy + dimensions_loss + classification_loss)\n",
    "rate = tf.train.exponential_decay(rate_initial, batch*BATCH_SIZE,\n",
    "                                           20, 0.05, staircase=True)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation)\n",
    "\n",
    "correct_prediction = tf.equal(predicted_dimensions, ground_dims)\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.int64))\n",
    "\n",
    "correct_prediction_class = tf.equal(tf.argmax(logits_class, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation_class = tf.reduce_mean(tf.cast(correct_prediction_class, tf.float32))\n",
    "\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "class_loss = []\n",
    "dims_loss = []\n",
    "spatial_loss = []\n",
    "total_loss = []\n",
    "imgs = 0\n",
    "print(\"just before error\")\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = 10000\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        accuracy = 0\n",
    "        X_train, y_train, d_train, l_train = shuffle(data[:10000], masks_reshaped[:10000], dimensions, training_labels)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y, batch_d, batch_l= X_train[offset:end], y_train[offset:end], d_train[offset:end], l_train[offset:end]\n",
    "            \n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y, dims:batch_d, pred_label:batch_l})\n",
    "            \n",
    "            \n",
    "            \n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "\n",
    "        print(\"total loss\")\n",
    "        temp_loss = sess.run(loss_operation,feed_dict={x: batch_x, y: batch_y, dims:batch_d, pred_label:batch_l})\n",
    "        print(temp_loss)\n",
    "        total_loss.append(temp_loss)\n",
    "        \n",
    "        temp_dims_loss = sess.run(dimensions_loss,feed_dict={x: batch_x, y: batch_y, dims:batch_d, pred_label:batch_l})\n",
    "        dims_loss.append(temp_dims_loss)\n",
    "        \n",
    "        temp_cross_loss = sess.run(cross_entropy,feed_dict={x: batch_x, y: batch_y, dims:batch_d, pred_label:batch_l})\n",
    "        class_loss.append(temp_cross_loss)\n",
    "        \n",
    "        temp_spatial_loss = sess.run(classification_loss,feed_dict={x: batch_x, y: batch_y, dims:batch_d, pred_label:batch_l})\n",
    "        spatial_loss.append(temp_spatial_loss)\n",
    "        \n",
    "        print()\n",
    "\n",
    "    testing_accuracy = 0\n",
    "    for offset in range(10000, num_examples*2, BATCH_SIZE):\n",
    "        batch_x, batch_y, batch_d, batch_l= data[offset:end], masks_reshaped[offset:end], dimensions_stack[offset:end], testing_labels[offset:end]\n",
    "        testing_accuracy = testing_accuracy + sess.run(accuracy_operation_class, feed_dict={x: batch_x, y: batch_y, dims:batch_d, pred_label:batch_l})\n",
    "    \n",
    "    print(\"testing accuracy is: \", (testing_accuracy/100))\n",
    "    saver.save(sess, './q22Model')\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
