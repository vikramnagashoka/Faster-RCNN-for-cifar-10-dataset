{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = \"./data_batch_1\"\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "file1 = \"./data_batch_1\"\n",
    "files_dict1 = unpickle(file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_batch1 = files_dict1[b'labels']\n",
    "data_batch1 = files_dict1[b'data']\n",
    "filenames_batch1 = files_dict1[b'filenames']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Data Pre processing and functions to return BATCH_SIZE number of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "x = tf.placeholder(tf.float32, shape=[None, 32,32,3])\n",
    "y = tf.placeholder(tf.int32, shape=[None])\n",
    "one_hot_y = tf.one_hot(y, 10)\n",
    "def batch_norm(x, is_train=True, decay=0.99, epsilon=0.001):\n",
    "    shape_x = x.get_shape().as_list()\n",
    "    beta = tf.get_variable('beta', shape_x[-1], initializer=tf.constant_initializer(0.0))\n",
    "    gamma = tf.get_variable('gamma', shape_x[-1], initializer=tf.constant_initializer(1.0))\n",
    "    moving_mean = tf.get_variable('moving_mean', shape_x[-1],\n",
    "                initializer=tf.constant_initializer(0.0), trainable=False)\n",
    "    moving_var = tf.get_variable('moving_var', shape_x[-1],\n",
    "               initializer=tf.constant_initializer(1.0), trainable=False)\n",
    "\n",
    "    if is_train:\n",
    "        mean, var = tf.nn.moments(x, np.arange(len(shape_x)-1), keep_dims=True)\n",
    "        mean = tf.reshape(mean, [mean.shape.as_list()[-1]])\n",
    "        #mean = tf.reshape(mean, [mean.get_shape().as_list()[-1]])\n",
    "        var = tf.reshape(var, [var.shape.as_list()[-1]])\n",
    "\n",
    "        update_moving_mean = tf.assign(moving_mean, moving_mean*decay + mean*(1-decay))\n",
    "        update_moving_var = tf.assign(moving_var,\n",
    "                            moving_var*decay + shape_x[0]/(shape_x[0]-1)*var*(1-decay))\n",
    "        update_ops = [update_moving_mean, update_moving_var]\n",
    "\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            return tf.nn.batch_normalization(x, mean, var, beta, gamma, epsilon)\n",
    "\n",
    "    else:\n",
    "        mean = moving_mean\n",
    "        var = moving_var\n",
    "        return tf.nn.batch_normalization(x, mean, var, beta, gamma, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_test = \"./test_batch\"\n",
    "files_dict_test = unpickle(file_test)\n",
    "\n",
    "labels_batch_test = files_dict_test[b'labels']\n",
    "data_batch_test = files_dict_test[b'data']\n",
    "filenames_batch_test = files_dict_test[b'filenames']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def return_test_images(offset, end):\n",
    "    data_test = []\n",
    "    for i in range(offset, end):\n",
    "        print(i)\n",
    "        name = filenames_batch_test[i]\n",
    "\n",
    "        img = np.zeros((32, 32, 3), dtype=np.uint8)\n",
    "        r = (data_batch_test[i])[0:1024]\n",
    "\n",
    "        r = np.array(r).reshape(32,32)\n",
    "        g = (data_batch_test[i])[1024:2048]\n",
    "\n",
    "        g = np.array(g).reshape(32,32)\n",
    "        b = (data_batch_test[i])[2048:3072]\n",
    "\n",
    "        b = np.array(b).reshape(32,32)\n",
    "        im = np.array([r,g,b])\n",
    "\n",
    "        rindex = 0\n",
    "        gindex = 0\n",
    "        bindex = 0\n",
    "        for k in range(0,3):\n",
    "            for i in range(0,32):\n",
    "                for j in range(0,32):\n",
    "                    if k == 0:\n",
    "                        img[i][j][k] = r[i][j]\n",
    "                        rindex = rindex + 1\n",
    "                    if k == 1:\n",
    "                        img[i][j][k] = g[i][j]\n",
    "                        gindex = gindex + 1\n",
    "                    if k == 2:\n",
    "                        img[i][j][k] = b[i][j]\n",
    "                        bindex = bindex + 1\n",
    "\n",
    "        data_test.append(img)\n",
    "    return data_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import flatten\n",
    "from tensorflow.contrib.layers.python.layers import batch_norm as batch_norm\n",
    "slim = tf.contrib.slim\n",
    "mu = 0\n",
    "sigma = 0.1\n",
    "\n",
    "\n",
    "def cifarNet(x, is_train=True):\n",
    "    vs = tf.get_variable_scope()\n",
    "    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 3, 32), mean = mu, stddev = sigma))\n",
    "    conv1_b = tf.Variable(tf.zeros(32))\n",
    "    conv1   = tf.nn.conv2d(x, conv1_W, strides=[1, 2, 2, 1], padding='SAME') + conv1_b\n",
    "    conv1 = batch_norm(conv1)\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    #Layer 2\n",
    "    conv2_b = tf.Variable(tf.zeros(64))\n",
    "    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 32, 64), mean = mu, stddev = sigma))\n",
    "    conv2   = tf.nn.conv2d(conv1, conv2_W, strides=[1, 2, 2, 1], padding='SAME') + conv2_b\n",
    "    conv2 = batch_norm(conv2)\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "    #Layer 3\n",
    "    conv3_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 64, 128), mean = mu, stddev = sigma))\n",
    "    conv3_b = tf.Variable(tf.zeros(128))\n",
    "    conv3   = tf.nn.conv2d(conv2, conv3_W, strides=[1, 2, 2, 1], padding='SAME') + conv3_b\n",
    "    conv3 = batch_norm(conv3)\n",
    "    conv3 = tf.nn.relu(conv3)\n",
    "\n",
    "    conv3 = tf.nn.max_pool(conv3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    #Layer 4\n",
    "    \n",
    "    conv4_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 128, 256), mean = mu, stddev = sigma))\n",
    "    conv4_b = tf.Variable(tf.zeros(256))\n",
    "    \n",
    "    conv4   = tf.nn.conv2d(conv3, conv4_W, strides=[1, 2, 2, 1], padding='SAME') + conv4_b\n",
    "    conv4 = batch_norm(conv4)\n",
    "    \n",
    "    conv4 = tf.nn.relu(conv4) \n",
    "    conv4 = tf.nn.max_pool(conv4, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    #Layer 5\n",
    "    conv5_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 256, 512), mean = mu, stddev = sigma))\n",
    "    conv5_b = tf.Variable(tf.zeros(512))\n",
    "    \n",
    "    conv5   = tf.nn.conv2d(conv4, conv5_W, strides=[1, 2, 2, 1], padding='SAME') + conv5_b\n",
    "    conv5 = batch_norm(conv5)\n",
    "    conv5 = tf.nn.relu(conv5)\n",
    "    conv5 = tf.nn.max_pool(conv5, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    #FC layer\n",
    "    fc1   = flatten(conv5)\n",
    "    fc1_W  = tf.Variable(tf.truncated_normal(shape=(512, 10), mean = mu, stddev = sigma))\n",
    "    fc1_b  = tf.Variable(tf.zeros(10))\n",
    "    logits = tf.matmul(fc1, fc1_W) + fc1_b\n",
    "\n",
    "    return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape data to (32,32,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data_train = []\n",
    "\n",
    "for i in range(len(filenames_batch1)): #\n",
    "    name = filenames_batch1[i]\n",
    "\n",
    "    name = str(name)\n",
    "    name = name[2:-1]\n",
    "\n",
    "    img = np.zeros((32, 32, 3), dtype=np.uint8)\n",
    "    r = (data_batch1[i])[0:1024]\n",
    "\n",
    "    r = np.array(r).reshape(32,32)\n",
    "    g = (data_batch1[i])[1024:2048]\n",
    "\n",
    "    g = np.array(g).reshape(32,32)\n",
    "    b = (data_batch1[i])[2048:3072]\n",
    "\n",
    "    b = np.array(b).reshape(32,32)\n",
    "    im = np.array([r,g,b])\n",
    "\n",
    "    rindex = 0\n",
    "    gindex = 0\n",
    "    bindex = 0\n",
    "    for k in range(0,3):\n",
    "        for i in range(0,32):\n",
    "            for j in range(0,32):\n",
    "                if k == 0:\n",
    "                    img[i][j][k] = r[i][j]\n",
    "                    rindex = rindex + 1\n",
    "                if k == 1:\n",
    "                    img[i][j][k] = g[i][j]\n",
    "                    gindex = gindex + 1\n",
    "                if k == 2:\n",
    "                    img[i][j][k] = b[i][j]\n",
    "                    bindex = bindex + 1\n",
    "    \n",
    "    data_train.append(img)\n",
    "\n",
    "data_train = np.asarray(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "BATCH_SIZE = 1000\n",
    "EPOCHS = 2000\n",
    "rate_initial = 0.001\n",
    "is_train = True\n",
    "batch = tf.Variable(0)\n",
    "\n",
    "logits = cifarNet(x, is_train)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "rate = tf.train.exponential_decay(rate_initial, batch*BATCH_SIZE,\n",
    "                                           2000, 0.05, staircase=True)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "accuracy_training = []\n",
    "accuracy_test = []\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = 10000\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        X_train, y_train = shuffle(data_b1, labels_batch1)\n",
    "        #print (\"shape is \" + str(X_train[i].shape))\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        print(\"Training Accuracy = {:.3f}\".format(accuracy))\n",
    "        print()\n",
    "    test_accuracy = 0\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        end = offset + BATCH_SIZE\n",
    "        batch_x, batch_y = data_test[offset:end], labels_batch_test[offset:end]\n",
    "        test_accuracy = test_accuracy + sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "    \n",
    "    print(\"test accuracy is: \", (test_accuracy/100))\n",
    "        \n",
    "        \n",
    "    saver.save(sess, './cifarNetModel')\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import flatten\n",
    "from tensorflow.contrib.layers.python.layers import batch_norm as batch_norm\n",
    "slim = tf.contrib.slim\n",
    "mu = 0\n",
    "sigma = 0.1\n",
    "\n",
    "\n",
    "def cifarNetb(x, is_train=True):\n",
    "    vs = tf.get_variable_scope()\n",
    "    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 3, 32), mean = mu, stddev = sigma))\n",
    "    conv1_b = tf.Variable(tf.zeros(96))\n",
    "\n",
    "    conv1 = tf.nn.depthwise_conv2d(x, conv1_W, strides=[1, 2, 2, 1], padding='VALID') + conv1_b\n",
    "    conv1 = batch_norm(conv1)\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    \n",
    "    # 1x1 layer\n",
    "    \n",
    "    conv1_W_s = tf.Variable(tf.truncated_normal(shape=(1, 1, 96, 1), mean = mu, stddev = sigma))\n",
    "    conv1_b_s = tf.Variable(tf.zeros(1))\n",
    "    conv1_s   = tf.nn.conv2d(conv1, conv1_W_s, strides=[1, 2, 2, 1], padding='VALID') + conv1_b_s\n",
    "    conv1_s = batch_norm(conv1_s)\n",
    "    conv1_s = tf.nn.relu(conv1_s)\n",
    "    \n",
    "    #Layer 2\n",
    "\n",
    "    conv2_b = tf.Variable(tf.zeros(64))\n",
    "\n",
    "    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 1, 64), mean = mu, stddev = sigma))\n",
    "\n",
    "    conv2 = tf.nn.depthwise_conv2d(conv1_s, conv2_W, strides=[1, 2, 2, 1], padding='VALID') + conv2_b\n",
    "    conv2 = batch_norm(conv2)\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "\n",
    "    \n",
    "    # 1x1 layer\n",
    "    \n",
    "    conv2_W_s = tf.Variable(tf.truncated_normal(shape=(1, 1, 64, 1), mean = mu, stddev = sigma))\n",
    "    conv2_b_s = tf.Variable(tf.zeros(1))\n",
    "    conv2_s   = tf.nn.conv2d(conv2, conv2_W_s, strides=[1, 2, 2, 1], padding='VALID') + conv2_b_s\n",
    "    conv2_s = batch_norm(conv2_s)\n",
    "    conv2_s = tf.nn.relu(conv2_s)\n",
    "    \n",
    "    #Layer 3\n",
    "\n",
    "    conv3_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 1, 128), mean = mu, stddev = sigma))\n",
    "\n",
    "    conv3_b = tf.Variable(tf.zeros(128))\n",
    "\n",
    "    conv3 = tf.nn.depthwise_conv2d(conv2_s, conv3_W, strides=[1, 2, 2, 1], padding='VALID') + conv3_b\n",
    "    conv3 = batch_norm(conv3)\n",
    "    conv3 = tf.nn.relu(conv3)\n",
    "\n",
    "    \n",
    "    # 1x1 layer\n",
    "    \n",
    "    conv3_W_s = tf.Variable(tf.truncated_normal(shape=(1, 1, 128, 1), mean = mu, stddev = sigma))\n",
    "    conv3_b_s = tf.Variable(tf.zeros(1))\n",
    "    conv3_s   = tf.nn.conv2d(conv3, conv3_W_s, strides=[1, 2, 2, 1], padding='VALID') + conv3_b_s\n",
    "    conv3_s = batch_norm(conv3_s)\n",
    "    conv3_s = tf.nn.relu(conv3_s)\n",
    "    \n",
    "    #Layer 4\n",
    "    \n",
    "\n",
    "    conv4_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 1, 256), mean = mu, stddev = sigma))\n",
    "\n",
    "    conv4_b = tf.Variable(tf.zeros(256))\n",
    "    \n",
    "\n",
    "    conv4 = tf.nn.depthwise_conv2d(conv3_s, conv4_W, strides=[1, 2, 2, 1], padding='VALID') + conv4_b\n",
    "    conv4 = batch_norm(conv4)\n",
    "    conv4 = tf.nn.relu(conv4) \n",
    "\n",
    "    # 1x1 layer\n",
    "    \n",
    "    conv4_W_s = tf.Variable(tf.truncated_normal(shape=(1, 1, 256, 1), mean = mu, stddev = sigma))\n",
    "    conv4_b_s = tf.Variable(tf.zeros(1))\n",
    "    conv4_s   = tf.nn.conv2d(conv4, conv4_W_s, strides=[1, 2, 2, 1], padding='VALID') + conv4_b_s\n",
    "    conv4_s = batch_norm(conv4_s)\n",
    "    conv4_s = tf.nn.relu(conv4_s)\n",
    "    \n",
    "    \n",
    "    #Layer 5\n",
    "    conv5_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 1, 512), mean = mu, stddev = sigma))\n",
    "\n",
    "    conv5_b = tf.Variable(tf.zeros(512))\n",
    "    \n",
    "    conv5   = tf.nn.conv2d(conv4_s, conv5_W, strides=[1, 2, 2, 1], padding='VALID') + conv5_b\n",
    "\n",
    "    conv5 = batch_norm(conv5)\n",
    "    conv5 = tf.nn.relu(conv5)\n",
    "    conv5 = tf.nn.max_pool(conv5, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    #FC layer\n",
    "    fc1   = flatten(conv5)\n",
    "    fc1_W  = tf.Variable(tf.truncated_normal(shape=(512, 10), mean = mu, stddev = sigma))\n",
    "    fc1_b  = tf.Variable(tf.zeros(10))\n",
    "    logits = tf.matmul(fc1, fc1_W) + fc1_b\n",
    "\n",
    "    return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "EPOCHS = 2000\n",
    "rate_initial = 0.001\n",
    "is_train = True\n",
    "batch = tf.Variable(0)\n",
    "\n",
    "logits = cifarNetb(x, is_train)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "rate = tf.train.exponential_decay(rate_initial, batch*BATCH_SIZE,\n",
    "                                           2000, 0.05, staircase=True)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "accuracy_training = []\n",
    "accuracy_test = []\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = 10000\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        X_train, y_train = shuffle(data_train, labels_batch1)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "    \n",
    "    test_accuracy = 0\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        end = offset + BATCH_SIZE\n",
    "        batch_x = return_test_images(offset,end)\n",
    "        batch_y= labels_batch_test[offset:end]\n",
    "        test_accuracy = test_accuracy + sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "    \n",
    "    print(\"test accuracy is: \", (test_accuracy/100))\n",
    "        \n",
    "    saver.save(sess, './cifarNetModel')\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 c) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import flatten\n",
    "from tensorflow.contrib.layers.python.layers import batch_norm as batch_norm\n",
    "slim = tf.contrib.slim\n",
    "mu = 0\n",
    "sigma = 0.1\n",
    "\n",
    "\n",
    "def cifarNetc(x, is_train=True):\n",
    "    vs = tf.get_variable_scope()\n",
    "    conv1_W = tf.Variable(tf.truncated_normal(shape=(3, 3, 3, 32), mean = mu, stddev = sigma))\n",
    "    conv1_b = tf.Variable(tf.zeros(32))\n",
    "    conv1   = tf.nn.conv2d(x, conv1_W, strides=[1, 2, 2, 1], padding='VALID') + conv1_b\n",
    "    conv1 = batch_norm(conv1)\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "\n",
    "    #residual 1\n",
    "    conv1_r_W = tf.Variable(tf.truncated_normal(shape=(3, 3, 32, 32), mean = mu, stddev = sigma))\n",
    "    conv1_r_b = tf.Variable(tf.zeros(32))\n",
    "    conv1_r   = tf.nn.conv2d(conv1, conv1_r_W, strides=[1, 2, 2, 1], padding='VALID') + conv1_r_b\n",
    "    conv1_r = batch_norm(conv1_r)\n",
    "    \n",
    "\n",
    "    #Layer 2\n",
    "    conv2_b = tf.Variable(tf.zeros(64))\n",
    "    conv2_W = tf.Variable(tf.truncated_normal(shape=(3, 3, 32, 64), mean = mu, stddev = sigma))\n",
    "    conv2   = tf.nn.conv2d(conv1_r, conv2_W, strides=[1, 2, 2, 1], padding='VALID') + conv2_b\n",
    "    conv2 = batch_norm(conv2)\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    #conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    #residual 2\n",
    "    conv2_r_W = tf.Variable(tf.truncated_normal(shape=(3, 3, 64, 64), mean = mu, stddev = sigma))\n",
    "    conv2_r_b = tf.Variable(tf.zeros(64))\n",
    "    conv2_r   = tf.nn.conv2d(conv2, conv2_r_W, strides=[1, 2, 2, 1], padding='VALID') + conv2_r_b\n",
    "    conv2_r = batch_norm(conv2_r)\n",
    "    \n",
    "    #Layer 3\n",
    "    conv3_W = tf.Variable(tf.truncated_normal(shape=(3, 3, 64, 128), mean = mu, stddev = sigma))\n",
    "    conv3_b = tf.Variable(tf.zeros(128))\n",
    "    conv3   = tf.nn.conv2d(conv2_r, conv3_W, strides=[1, 2, 2, 1], padding='VALID') + conv3_b\n",
    "    conv3 = batch_norm(conv3)\n",
    "    conv3 = tf.nn.relu(conv3)\n",
    "\n",
    "    #conv3 = tf.nn.max_pool(conv3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    #residual 3\n",
    "    conv3_r_W = tf.Variable(tf.truncated_normal(shape=(3, 3, 128, 128), mean = mu, stddev = sigma))\n",
    "    conv3_r_b = tf.Variable(tf.zeros(128))\n",
    "    conv3_r   = tf.nn.conv2d(conv3, conv3_r_W, strides=[1, 2, 2, 1], padding='VALID') + conv3_r_b\n",
    "    conv3_r = batch_norm(conv3_r)\n",
    "    \n",
    "    #Layer 4\n",
    "    \n",
    "    conv4_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 128, 256), mean = mu, stddev = sigma))\n",
    "    conv4_b = tf.Variable(tf.zeros(256))\n",
    "    \n",
    "    conv4   = tf.nn.conv2d(conv3_r, conv4_W, strides=[1, 2, 2, 1], padding='VALID') + conv4_b\n",
    "    conv4 = batch_norm(conv4)\n",
    "    \n",
    "    conv4 = tf.nn.relu(conv4) \n",
    "    #conv4 = tf.nn.max_pool(conv4, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    #residual 4\n",
    "    conv4_r_W = tf.Variable(tf.truncated_normal(shape=(3, 3, 256, 256), mean = mu, stddev = sigma))\n",
    "    conv4_r_b = tf.Variable(tf.zeros(256))\n",
    "    conv4_r   = tf.nn.conv2d(conv4, conv4_r_W, strides=[1, 2, 2, 1], padding='VALID') + conv4_r_b\n",
    "    conv4_r = batch_norm(conv4_r)\n",
    "    \n",
    "    \n",
    "    #Layer 5\n",
    "    conv5_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 256, 512), mean = mu, stddev = sigma))\n",
    "    conv5_b = tf.Variable(tf.zeros(512))\n",
    "    \n",
    "    conv5   = tf.nn.conv2d(conv4_r, conv5_W, strides=[1, 2, 2, 1], padding='VALID') + conv5_b\n",
    "    conv5 = batch_norm(conv5)\n",
    "    conv5 = tf.nn.relu(conv5)\n",
    "    conv5 = tf.nn.max_pool(conv5, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    #FC layer\n",
    "    fc1   = flatten(conv5)\n",
    "    fc1_W  = tf.Variable(tf.truncated_normal(shape=(512, 10), mean = mu, stddev = sigma))\n",
    "    fc1_b  = tf.Variable(tf.zeros(10))\n",
    "    logits = tf.matmul(fc1, fc1_W) + fc1_b\n",
    "    #prob = tf.nn.softmax(logits)\n",
    "    return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "EPOCHS = 2000\n",
    "rate_initial = 0.001\n",
    "is_train = True\n",
    "batch = tf.Variable(0)\n",
    "\n",
    "logits = cifarNetc(x, is_train)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "rate = tf.train.exponential_decay(rate_initial, batch*BATCH_SIZE,\n",
    "                                           2000, 0.05, staircase=True)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "\n",
    "accuracy_training = []\n",
    "accuracy_test = []\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = 10000\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        X_train, y_train = shuffle(data_train, labels_batch1)\n",
    "\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        print(\"Training Accuracy = {:.3f}\".format(accuracy))\n",
    "        print()\n",
    "\n",
    "    test_accuracy = 0\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        end = offset + BATCH_SIZE\n",
    "        batch_x = return_test_images(offset,end)\n",
    "        batch_y= labels_batch_test[offset:end]\n",
    "        test_accuracy = test_accuracy + sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "    \n",
    "    print(\"test accuracy is: \", (test_accuracy/100))\n",
    "        \n",
    "    saver.save(sess, './cifarNetModel')\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
