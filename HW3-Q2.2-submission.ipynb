{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "obj_centre = []\n",
    "obj_width = []\n",
    "lines = []\n",
    "lines = [line.rstrip('\\n') for line in open(\"./cifar10_transformed/devkit/train.txt\")]\n",
    "obj_centre = []\n",
    "obj_width = []\n",
    "\n",
    "\n",
    "lines = np.asarray(lines)\n",
    "print(lines[0])\n",
    "for i in range(len(lines)):\n",
    "    obj_centre.append([lines[i][12:15],lines[i][15:18]])\n",
    "    obj_width.append(lines[i][18:])\n",
    "\n",
    "obj_centre = np.asarray(obj_centre)\n",
    "obj_width = np.asarray(obj_width)\n",
    "print(obj_centre.shape)\n",
    "print(obj_width.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "from scipy import misc\n",
    "files = glob.glob(\"./cifar10_transformed/masks/*.png\")\n",
    "masks = []\n",
    "for i in range(len(files)):\n",
    "    temp_mask = misc.imread(files[i])\n",
    "    masks.append(temp_mask)\n",
    "masks_reshaped = []\n",
    "for i in range(len(masks)):\n",
    "    masks_reshaped.append(np.ravel(masks[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "file_data = glob.glob(\"./cifar10_transformed/imgs/*.png\")\n",
    "for i in range(len(file_data)):\n",
    "    name = file_data[i]\n",
    "    data.append(misc.imread(name))\n",
    "\n",
    "data = np.asarray(data)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "lines = [line.rstrip('\\n') for line in open(\"./cifar10_transformed/devkit/train.txt\")]\n",
    "rows = []\n",
    "columns = []\n",
    "width = []\n",
    "\n",
    "\n",
    "lines = np.asarray(lines)\n",
    "print(lines[0])\n",
    "for i in range(len(lines)):\n",
    "    rows.append(lines[i][12:15])\n",
    "    columns.append(lines[i][15:18])\n",
    "    width.append(lines[i][18:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = np.vstack([rows,columns,width])\n",
    "print(dimensions.shape)\n",
    "dimensions = dimensions.transpose()\n",
    "print(dimensions[1000][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "lines = [line.rstrip('\\n') for line in open(\"./cifar10_transformed/devkit/test.txt\")]\n",
    "rows = []\n",
    "columns = []\n",
    "width = []\n",
    "\n",
    "\n",
    "lines = np.asarray(lines)\n",
    "print(lines[0])\n",
    "for i in range(len(lines)):\n",
    "    rows.append(lines[i][12:15])\n",
    "    columns.append(lines[i][15:18])\n",
    "    width.append(lines[i][18:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions_test = np.vstack([rows,columns,width]).transpose()\n",
    "print(dimensions_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "x = tf.placeholder(tf.float32, shape=[None, 48,48,3])\n",
    "\n",
    "def batch_norm(x, is_train=True, decay=0.99, epsilon=0.001):\n",
    "    shape_x = x.get_shape().as_list()\n",
    "    beta = tf.get_variable('beta', shape_x[-1], initializer=tf.constant_initializer(0.0))\n",
    "    gamma = tf.get_variable('gamma', shape_x[-1], initializer=tf.constant_initializer(1.0))\n",
    "    moving_mean = tf.get_variable('moving_mean', shape_x[-1],\n",
    "                initializer=tf.constant_initializer(0.0), trainable=False)\n",
    "    moving_var = tf.get_variable('moving_var', shape_x[-1],\n",
    "               initializer=tf.constant_initializer(1.0), trainable=False)\n",
    "\n",
    "    if is_train:\n",
    "        mean, var = tf.nn.moments(x, np.arange(len(shape_x)-1), keep_dims=True)\n",
    "        mean = tf.reshape(mean, [mean.shape.as_list()[-1]])\n",
    "        var = tf.reshape(var, [var.shape.as_list()[-1]])\n",
    "\n",
    "        update_moving_mean = tf.assign(moving_mean, moving_mean*decay + mean*(1-decay))\n",
    "        update_moving_var = tf.assign(moving_var,\n",
    "                            moving_var*decay + shape_x[0]/(shape_x[0]-1)*var*(1-decay))\n",
    "        update_ops = [update_moving_mean, update_moving_var]\n",
    "\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            return tf.nn.batch_normalization(x, mean, var, beta, gamma, epsilon)\n",
    "\n",
    "    else:\n",
    "        mean = moving_mean\n",
    "        var = moving_var\n",
    "        return tf.nn.batch_normalization(x, mean, var, beta, gamma, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import flatten\n",
    "from tensorflow.contrib.layers.python.layers import batch_norm as batch_norm\n",
    "slim = tf.contrib.slim\n",
    "mu = 0\n",
    "sigma = 0.1\n",
    "\n",
    "img_index = 0\n",
    "def cifarNet(x, y, dims, is_train=True):\n",
    "    two = tf.constant(2,tf.float32)\n",
    "    one = tf.constant(1,tf.float32)\n",
    "    vs = tf.get_variable_scope()\n",
    "    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 3, 32), mean = mu, stddev = sigma))\n",
    "    conv1_b = tf.Variable(tf.zeros(32))\n",
    "    conv1   = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='SAME') + conv1_b\n",
    "    conv1 = batch_norm(conv1)\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    #Layer 2\n",
    "    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 32, 64), mean = mu, stddev = sigma))\n",
    "    conv2_b = tf.Variable(tf.zeros(64))\n",
    "    conv2   = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='SAME') + conv2_b\n",
    "    conv2 = batch_norm(conv2)\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    #Layer 3\n",
    "    conv3_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 64, 128), mean = mu, stddev = sigma))\n",
    "    conv3_b = tf.Variable(tf.zeros(128))\n",
    "    conv3   = tf.nn.conv2d(conv2, conv3_W, strides=[1, 1, 1, 1], padding='SAME') + conv3_b\n",
    "    \n",
    "    conv3 = batch_norm(conv3)\n",
    "    conv3 = tf.nn.relu(conv3)\n",
    "    conv3 = tf.nn.max_pool(conv3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    #Layer 4\n",
    "    \n",
    "    conv4_W = tf.Variable(tf.truncated_normal(shape=(3, 3, 128, 256), mean = mu, stddev = sigma))\n",
    "    conv4_b = tf.Variable(tf.zeros(256))\n",
    "    \n",
    "    conv4   = tf.nn.conv2d(conv3, conv4_W, strides=[1, 1, 1, 1], padding='SAME') + conv4_b\n",
    "    conv4 = batch_norm(conv4)\n",
    "    \n",
    "    conv4 = tf.nn.relu(conv4)\n",
    "    \n",
    "    #Layer 5 intermediate layer\n",
    "    conv5_W = tf.Variable(tf.truncated_normal(shape=(3, 3, 256, 256), mean = mu, stddev = sigma))\n",
    "    conv5_b = tf.Variable(tf.zeros(256))\n",
    "    \n",
    "    conv5   = tf.nn.conv2d(conv4, conv5_W, strides=[1, 1, 1, 1], padding='SAME') + conv5_b\n",
    "    conv5 = batch_norm(conv5)\n",
    "    conv5 = tf.nn.relu(conv5)\n",
    "    \n",
    "    #Layer 6 for classification\n",
    "    conv6_W = tf.Variable(tf.truncated_normal(shape=(1, 1, 256, 1), mean = mu, stddev = sigma))\n",
    "    conv6_b = tf.Variable(tf.zeros(1))\n",
    "    \n",
    "    conv6   = tf.nn.conv2d(conv5, conv6_W,strides=[1, 1, 1, 1], padding='SAME') + conv6_b\n",
    "    conv6 =  flatten(conv6)\n",
    "   \n",
    "    bool_indices = tf.not_equal(y, two)\n",
    "    threshold_indices = tf.where(bool_indices)\n",
    "    ground_truth = tf.gather_nd(y, threshold_indices)\n",
    "    predicted = tf.gather_nd(conv6,threshold_indices)\n",
    "    \n",
    "    #Layer 7 for regression\n",
    "    conv7_W = tf.Variable(tf.truncated_normal(shape=(1, 1, 256, 3), mean = mu, stddev = sigma))\n",
    "    conv7_b = tf.Variable([24,24,32], dtype=tf.float32)\n",
    "    \n",
    "    conv7   = tf.nn.conv2d(conv5, conv7_W,strides=[1, 1, 1, 1], padding='SAME') + conv7_b\n",
    "    \n",
    "\n",
    "    \n",
    "    bool_indices_ones = tf.equal(y, one)\n",
    "    threshold_indices_ones = tf.where(bool_indices_ones)\n",
    "    \n",
    "    conv7 = tf.reshape(conv7,[100,36,3])\n",
    "    predicted_rows = tf.slice(conv7,[0,0,0], [100,36,1])\n",
    "    predicted_rows = tf.reshape(predicted_rows,[100,36])\n",
    "\n",
    "    predicted_columns = tf.slice(conv7,[0,0,1],[100,36,1])\n",
    "    predicted_columns = tf.reshape(predicted_columns,[100,36])\n",
    "\n",
    "    \n",
    "    predicted_width = tf.slice(conv7,[0,0,2],[100,36,1])\n",
    "    predicted_width = tf.reshape(predicted_width,[100,36])\n",
    "\n",
    "    \n",
    "    predicted_rows_dimensions = tf.gather_nd(predicted_rows, threshold_indices_ones)\n",
    "    predicted_columns_dimensions = tf.gather_nd(predicted_columns, threshold_indices_ones)\n",
    "    predicted_width_dimensions = tf.gather_nd(predicted_width, threshold_indices_ones)\n",
    "    \n",
    "    anchor_size = tf.constant(32,dtype=tf.float32)\n",
    "    predicted_rows_dimensions = predicted_rows_dimensions / anchor_size\n",
    "\n",
    "    predicted_rows_dimensions = tf.reshape(predicted_rows_dimensions,[-1,1])\n",
    "    \n",
    "    predicted_columns_dimensions = predicted_columns_dimensions / anchor_size\n",
    "    predicted_columns_dimensions = tf.reshape(predicted_columns_dimensions,[-1,1])\n",
    "\n",
    "    \n",
    "    predicted_width_dimensions = predicted_width_dimensions / anchor_size\n",
    "    predicted_width_dimensions = tf.reshape(predicted_width_dimensions,[-1,1])\n",
    "\n",
    "    \n",
    "\n",
    "    row_dims = tf.slice(dims,[0,0],[100, 1])\n",
    "    row_dims = tf.tile(row_dims,[1,36])\n",
    "    rows_dims = tf.gather_nd(row_dims, threshold_indices_ones)\n",
    "    rows_dims = rows_dims / anchor_size\n",
    "    rows_dims = tf.reshape(rows_dims,[-1,1])\n",
    "    \n",
    "    \n",
    "    column_dims = tf.slice(dims,[0,1],[100, 1])\n",
    "    column_dims = tf.tile(column_dims,[1,36])\n",
    "    columns_dims = tf.gather_nd(column_dims, threshold_indices_ones)\n",
    "    columns_dims = columns_dims / anchor_size\n",
    "    columns_dims = tf.reshape(columns_dims,[-1,1])\n",
    "    \n",
    "    w_dims = tf.slice(dims,[0,2],[100, 1])\n",
    "    w_dims = tf.tile(w_dims,[1,36])\n",
    "    width_dims = tf.gather_nd(w_dims, threshold_indices_ones)\n",
    "    width_dims = width_dims / anchor_size\n",
    "    width_dims = tf.reshape(width_dims,[-1,1])\n",
    "    \n",
    "    ground_dims = tf.concat([rows_dims, columns_dims,width_dims], axis=1)\n",
    "    \n",
    "\n",
    "    predicted_dimensions = tf.concat([predicted_rows_dimensions, predicted_columns_dimensions, predicted_width_dimensions],axis=1)\n",
    "\n",
    "\n",
    "    \n",
    "    return conv6, ground_truth,predicted, predicted_dimensions, ground_dims\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "EPOCHS = 2000\n",
    "rate_initial = 0.001\n",
    "is_train = True\n",
    "batch = tf.Variable(0)\n",
    "losses_training = []\n",
    "y = tf.placeholder(tf.float32, shape=[None,36])\n",
    "w = tf.placeholder(tf.float32, shape=[None,6,6])\n",
    "dims = tf.placeholder(tf.float32, shape=[None,3])\n",
    "ground_dimensions = tf.placeholder(tf.float32, shape=[None,6,3])\n",
    "logits, ground_truth ,predicted, predicted_dimensions,ground_dims = cifarNet(x, y,dims, is_train)\n",
    "\n",
    "\n",
    "predicted_temp = tf.nn.sigmoid(predicted)\n",
    "predicted_threshold = tf.greater(predicted_temp, 0.5)\n",
    "predicted_threshold = tf.cast(predicted_threshold, tf.int32)\n",
    "ground_truth_acc = tf.nn.sigmoid(ground_truth)\n",
    "predicted_threshold = tf.cast(predicted_threshold, tf.float32)\n",
    "\n",
    "\n",
    "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=ground_truth, logits=predicted)\n",
    "\n",
    "dimensions_loss = tf.losses.huber_loss(predicted_dimensions, ground_dims)\n",
    "\n",
    "\n",
    "loss_operation = tf.reduce_mean(cross_entropy + dimensions_loss)\n",
    "rate = tf.train.exponential_decay(rate_initial, batch*BATCH_SIZE,\n",
    "                                           20, 0.05, staircase=True)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation)\n",
    "\n",
    "correct_prediction = tf.equal(predicted_dimensions, ground_dims)\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.int64))\n",
    "\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = 10000\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        accuracy = 0\n",
    "        X_train, y_train, d_train = (data, masks_reshaped, dimensions)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y, batch_d= X_train[offset:end], y_train[offset:end], d_train[offset:end]\n",
    "            \n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y, dims:batch_d})\n",
    "            \n",
    "            accuracy = accuracy + sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y,dims:batch_d})\n",
    "            break\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"loss\")\n",
    "        temp_loss = sess.run(loss_operation,feed_dict={x: batch_x, y: batch_y, dims:batch_d})\n",
    "        print(temp_loss)\n",
    "        losses_training.append(temp_loss)\n",
    "        print()\n",
    "\n",
    "    print(\"testing accuracy\")\n",
    "    print(sess.run(loss_operation,feed_dict={x: data[num_examples+1:], y: masks_reshaped[num_examples+1:], dims:dimensions_test}))\n",
    "        \n",
    "    saver.save(sess, './q22Model')\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
